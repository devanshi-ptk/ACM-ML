{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1. DATA EXPLORATION AND UNDERSTANDING\n",
        "1.   loaded the datasets\n",
        "2.   printed the no. of rows and columns, the data type of each feature and no. of missing values in each feature\n",
        "\n"
      ],
      "metadata": {
        "id": "UGOxc4NIBnNS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SUHrgqzRexUj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#LOAD AND EXPLORE DATA\n",
        "import pandas as pd\n",
        "train = pd.read_csv('train.csv')\n",
        "test = pd.read_csv('test.csv')\n",
        "\n",
        "#---structure and missing values---\n",
        "print(\"=== Train Dataset Info ===\")\n",
        "print(\"Shape:\", train.shape)\n",
        "print(\"\\nData Types:\\n\", train.dtypes)\n",
        "print(\"\\nMissing Values:\\n\", train.isnull().sum())\n",
        "\n",
        "print(\"\\n=== Test Dataset Info ===\")\n",
        "print(\"Shape:\", test.shape)\n",
        "print(\"\\nData Types:\\n\", test.dtypes)\n",
        "print(\"\\nMissing Values:\\n\", test.isnull().sum())\n",
        "\n",
        "#statistical summary\n",
        "print(\"\\n=== Train Head ===\")\n",
        "print(train.head(3))\n",
        "\n",
        "print(\"\\n=== Train Describe ===\")\n",
        "print(train.describe(include='all'))  # 'include=all' for categorical as well\n",
        "\n",
        "print(\"\\n=== Test Head ===\")\n",
        "print(test.head(3))\n",
        "\n",
        "print(\"\\n=== Test Describe ===\")\n",
        "print(test.describe(include='all'))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **categorical data**(*data that can be grouped into categories instead of measuring numerically*) is visualised using bar plots --> [Survival vs Pclass,Gender vs Survival].\n",
        "2.   **numeric data** is visualised using histograms/KDE plots --> [Age distribution of survivors vs non-survivors, Fare distribution]\n",
        "3.   **missing values** are quickly visualised using heatmaps\n",
        "\n"
      ],
      "metadata": {
        "id": "c68tgPo5EmYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#VISUALIZE KEY PATTERNS\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Survival rate by passenger class\n",
        "sns.barplot(x='Pclass', y='Survived', data=train)\n",
        "plt.show()\n",
        "\n",
        "# Age distribution of survivors vs non-survivors\n",
        "sns.histplot(data=train, x='Age', hue='Survived', kde=True, bins=30)\n",
        "plt.show()\n",
        "\n",
        "# Gender-based survival analysis\n",
        "sns.barplot(x='Sex', y='Survived', data=train)\n",
        "plt.show()\n",
        "\n",
        "# Fare distribution analysis\n",
        "sns.histplot(data=train, x='Fare', hue='Survived', bins=30)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "L2LYAHBZCiwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap for Missing Data\n",
        "sns.heatmap(train.isnull(), cbar=False)\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7S2i0crWDzRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. DATA PRE-PROCESSING"
      ],
      "metadata": {
        "id": "9nHNU_Oeap0r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.1 MISSING DATA ANALYSIS\n",
        "1. to identify missing values we check which columns have 'NaN' and how many.\\\n",
        "   missing --> age,cabin,embarked,fare\n",
        "2. visualisation --> heatmap used\n",
        "3. **Age** - around 20% missing. Missing values are replaced by *median* of the column\n",
        "4. **Embarked** - 2 values missing. Replaced by *mode* of column.\n",
        "5. **Cabin** - 70% missing. Either *drop* the column or *create a feature* 'HasCabin'(1=yes,0=no)\n",
        "6. **Fare** - 1 missing in test data. The missing value can be replaced with *median* of the column"
      ],
      "metadata": {
        "id": "nn_M2dnYIuLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#HANDLING MISSING VALUES\n",
        "#filling missing data for 'Age'\n",
        "train['Age'] = train['Age'].fillna(train['Age'].median())\n",
        "test['Age'] = test['Age'].fillna(test['Age'].median())\n",
        "\"\"\"\n",
        "inplace directly modifies the dataframe without the need to assign it back\n",
        "\"\"\"\n",
        "\n",
        "#filling missing data for 'Embarked'\n",
        "train['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode()[0])\n",
        "\"\"\"\n",
        "---No missing 'Embarked' in test data---\n",
        "mode returns series of values. \".mode()[0]\" only returns the first element.\n",
        "\"\"\"\n",
        "\n",
        "#Creating New Binary feature for known cabins\n",
        "train['Cabin_known'] = train['Cabin'].notnull().astype(int)\n",
        "test['Cabin_known'] = test['Cabin'].notnull().astype(int)\n",
        "\"\"\"\n",
        "Cabin_known is the new feature.\n",
        "\".notnull()\" returns a boolean(true/false) depending on whether 'Cabin' had a value or not.\n",
        "Later, \".astype(int)\" converts this boolean to integers (true=1,false=0)\n",
        "This way the new feature has values 0/1.\n",
        "Cabin_known\n",
        "1\n",
        "0\n",
        "1\n",
        ".....\n",
        "\"\"\"\n",
        "\n",
        "# Drop the original Cabin column after creating Cabin_known\n",
        "train.drop('Cabin', axis=1, inplace=True)\n",
        "test.drop('Cabin', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "#filling missing data for 'Fare' in test dataset\n",
        "test['Fare'].fillna(test['Fare'].median(), inplace=True) # only one missing value in test set\n"
      ],
      "metadata": {
        "id": "ZSsr39Year7n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 FEATURE ENGINEERING\n",
        "1. We extract titles from the names using python `split` operation\n",
        "2. New feature named `FamilySize` is created.\n",
        "3. Continuous variables like 'Age' are grouped"
      ],
      "metadata": {
        "id": "3OaDyI_itbzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FEATURE ENGINEERING\n",
        "\n",
        "#Titles\n",
        "train['Title'] = train['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
        "test['Title'] = test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())\n",
        "\"\"\"\n",
        "Logic: Braund, Mr. Owen Harris\n",
        "\n",
        "x.split(',')[1] → \" Mr. Owen Harris\"\n",
        "\n",
        ".split('.')[0] → \" Mr\"\n",
        "\n",
        ".strip() → \"Mr\"\n",
        "\"\"\"\n",
        "\n",
        "#Family Size\n",
        "train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n",
        "test['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n",
        "\"\"\"\n",
        "family size = sibling/spouse + parent/children + passenger(themselves)\n",
        "\"\"\"\n",
        "\n",
        "#Age Bins\n",
        "train['AgeBin'] = pd.cut(train['Age'], bins=[0, 12, 18, 35, 60, 100], labels=['Child','Teen','Young','Adult','Senior'])\n",
        "test['AgeBin'] = pd.cut(test['Age'], bins=[0, 12, 18, 35, 60, 100], labels=['Child','Teen','Young','Adult','Senior'])\n",
        "\"\"\"\n",
        "Splits the 'Age' category into intervals/bins and assigns names to those bins.\n",
        "Example:\n",
        "0–12 → children\n",
        "13–18 → teens\n",
        "19–35 → young adults\n",
        "36–60 → adults\n",
        "61–100 → seniors\n",
        "This is then stored in a new feature 'AgeBin'. Now instead of 'Age=4' it becomes 'Child'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "y3G_6BFetOhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3 ENCODING CATEGORICAL VARIABLES\n",
        "1. 'Sex' is converted to numerical(0/1)\n",
        "2. 'Embarked', 'Title', 'AgeBin' are One Hot Encoded"
      ],
      "metadata": {
        "id": "e6G7BMzcuKzh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ENCODING CATERGORICAL VARIABLES\n",
        "\n",
        "#converting sex to numerical(0/1)\n",
        "train['Sex'] = train['Sex'].map({'male':0, 'female':1})\n",
        "test['Sex'] = test['Sex'].map({'male':0, 'female':1})\n",
        "\"\"\"\n",
        "\".map({'male':0,'female':1})\" replaces 'male' with 0 and 'female' with 1, thus making 'Sex' numeric\n",
        "\"\"\"\n",
        "\n",
        "#One Hot Encoding 'Embarked', 'Title', 'AgeBin'\n",
        "\n",
        "# Adding dummy columns for 'Embarked','Title' and 'AgeBin', without removing the original\n",
        "Embarked_dummies = pd.get_dummies(train['Embarked'], prefix='Embarked', drop_first=True)\n",
        "Embarked_dummies = Embarked_dummies.astype(int)\n",
        "train = pd.concat([train, Embarked_dummies], axis=1)\n",
        "\n",
        "Title_dummies = pd.get_dummies(train['Title'], prefix='Title', drop_first=True)\n",
        "Title_dummies = Title_dummies.astype(int)\n",
        "train = pd.concat([train, Title_dummies], axis=1)\n",
        "\n",
        "AgeBin_dummies = pd.get_dummies(train['AgeBin'], prefix='AgeBin', drop_first=True)\n",
        "AgeBin_dummies = AgeBin_dummies.astype(int)\n",
        "train = pd.concat([train,AgeBin_dummies], axis=1)\n",
        "\n",
        "\"\"\"\n",
        "1. we make dummy variables for 'Embarked', 'Title' and 'AgeBin'.\n",
        "2. \"drop_first\" drops the first column of the dummy variable to avoid redundancy. [Say, Embarked_S = 0,Embarked_C = 0,then Embarked_Q = 1 automatically]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gcqTCb6YtUWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.4 FEATURE SELECTION"
      ],
      "metadata": {
        "id": "q5Q1o-P7uo_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FEATURE SELECTION\n",
        "\n",
        "#dropping irrelevant columns\n",
        "train.drop(['PassengerId','Name','Ticket'], axis=1, inplace=True)\n",
        "test.drop(['PassengerId','Name','Ticket'], axis=1, inplace=True)\n",
        "\"\"\"\n",
        "why irrelevant?\n",
        "PassengerId - just an identifier. no relation with survival\n",
        "Name - mostly crowded data. we already extracted 'Title'\n",
        "Ticket - messy alphanumeric values. not useful\n",
        "\n",
        "axis=1 --> removes columns not rows\n",
        "inplace=True --> directly modifies 'train' without the need to assign it back\n",
        "\"\"\"\n",
        "\n",
        "#Analyze Feature Correlations\n",
        "\n",
        "# Calculate the correlation matrix for numeric features\n",
        "corr_matrix = train.corr(numeric_only=True)\n",
        "\"\"\"\n",
        "numeric_only --> ignores object/categorical datatypes since correlation makes sense only for numbers\n",
        "This gives a big square matrix where each cell = correlation between 2 features.\n",
        "\"\"\"\n",
        "\n",
        "# Inspect correlations with the target 'Survived'\n",
        "surv_corr = corr_matrix['Survived'].sort_values(ascending=False)\n",
        "\"\"\"\n",
        "extraction of correlation of all features wrt 'Survived'\n",
        "sort_values --> Sorts them so we can see strongest positive/negative relationships.\n",
        "\"\"\"\n",
        "print(\"\\n=== Correlation of Features with 'Survived' ===\")\n",
        "print(surv_corr)\n",
        "\n",
        "\n",
        "# Visualize the full correlation matrix as a heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", square=True)\n",
        "plt.title(\"Feature Correlation Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Bar plot: Top 10 features most correlated with survival (excluding 'Survived' itself)\n",
        "top_corr = surv_corr.drop('Survived').head(10) #'Survived' dropped beacuse correlation with self = 1\n",
        "plt.figure(figsize=(8, 5))\n",
        "top_corr.plot(kind='barh')\n",
        "plt.title(\"Top 10 Feature Correlations with Survival\")\n",
        "plt.xlabel(\"Correlation with 'Survived'\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.gca().invert_yaxis()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OZ6PhdnwtYrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. RANDOM FOREST CLASSIFIER"
      ],
      "metadata": {
        "id": "KGLCLZIcuxIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.1 DECISION TREE IMPLEMENTATION\n",
        "We first implemented a custom `DecisionTree` class from scratch. Each tree learns by recursively splitting the dataset on features that maximize information gain, using the Gini impurity measure as a criterion for split quality.\n",
        "\n",
        "Key features of our decision tree implementation:\n",
        "1. **Recursive tree building:** The algorithm continues splitting into left/right subtrees until all leaves are pure or until the stopping criteria (maximum depth, minimum samples) are met.\n",
        "2. **Gini impurity calculation:** We use Gini impurity to measure how well each potential split separates the classes.\n",
        "3. **Best split selection:** At every node, the tree considers all possible features and thresholds, choosing the split that produces the most gain in purity.\n",
        "4. **Predictions:** For a given sample, the tree traverses down its branches based on feature values until it reaches a leaf, at which point the predicted class is returned."
      ],
      "metadata": {
        "id": "Br-6X9fUXHUz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DECISION TREE IMPLEMENTATION\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, max_depth=None, min_samples_split=2, criterion = 'gini', max_features=None):\n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.criterion=criterion\n",
        "        self.max_features=max_features\n",
        "        self.tree = None\n",
        "\n",
        "    def gini_impurity(self, y):\n",
        "        \"\"\"\n",
        "        Computes the Gini impurity for a vector y of class labels.\n",
        "        \"\"\"\n",
        "        m = len(y)\n",
        "        if m == 0:\n",
        "            return 0\n",
        "        class_counts = np.bincount(y)\n",
        "        probs = class_counts / m\n",
        "        return 1 - np.sum(probs ** 2)\n",
        "\n",
        "    def best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Finds the best feature and threshold to split at this node.\n",
        "        \"\"\"\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        features = np.arange(n_features)\n",
        "\n",
        "        if self.max_features is not None and self.max_features < n_features:\n",
        "            features = np.random.choice(n_features, self.max_features, replace=False)\n",
        "\n",
        "        best_gain, split_idx, split_thresh =0, None, None\n",
        "\n",
        "        for feature in features:\n",
        "            thresholds = np.unique(X[:, feature])\n",
        "            for thresh in thresholds:\n",
        "                left = y[X[:, feature] < thresh]\n",
        "                right = y[X[:, feature] >= thresh]\n",
        "                if len(left) == 0 or len(right) == 0:\n",
        "                    continue\n",
        "                curr_gain = self.information_gain(y, left, right)\n",
        "                if curr_gain > best_gain:\n",
        "                    best_gain = curr_gain\n",
        "                    split_idx = feature\n",
        "                    split_thresh = thresh\n",
        "        return split_idx, split_thresh, best_gain\n",
        "\n",
        "    def information_gain(self, parent, left, right):\n",
        "        \"\"\"\n",
        "        Parent and children y's are given.\n",
        "        \"\"\"\n",
        "        w_left = len(left) / len(parent)\n",
        "        w_right = len(right) / len(parent)\n",
        "        gain = self.gini_impurity(parent) - (w_left * self.gini_impurity(left) + w_right * self.gini_impurity(right))\n",
        "        return gain\n",
        "\n",
        "    def split_data(self, X, y, feature, threshold):\n",
        "        \"\"\"\n",
        "        Splits X, y into left/right groups based on feature and threshold.\n",
        "        \"\"\"\n",
        "        left_idxs = X[:, feature] < threshold\n",
        "        right_idxs = X[:, feature] >= threshold\n",
        "        return X[left_idxs], y[left_idxs], X[right_idxs], y[right_idxs]\n",
        "\n",
        "    def build_tree(self, X, y, depth):\n",
        "        \"\"\"\n",
        "        Recursively builds the tree.\n",
        "        \"\"\"\n",
        "        num_samples, num_features = X.shape\n",
        "        num_labels = len(np.unique(y))\n",
        "        # Stopping conditions\n",
        "        if (self.max_depth is not None and depth >= self.max_depth) or num_labels == 1 or num_samples < self.min_samples_split:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            return {'leaf': True, 'class': leaf_value}\n",
        "\n",
        "        # Find best split\n",
        "        feature, thresh, best_gain = self.best_split(X, y)\n",
        "        if feature is None or thresh is None:\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            return {'leaf': True, 'class': leaf_value}\n",
        "\n",
        "        # Split and recurse\n",
        "        X_left, y_left, X_right, y_right = self.split_data(X, y, feature, thresh)\n",
        "        left = self.build_tree(X_left, y_left, depth + 1)\n",
        "        right = self.build_tree(X_right, y_right, depth + 1)\n",
        "        return {'leaf': False, 'feature': feature, 'thresh': thresh, 'left': left, 'right': right, 'gain':best_gain}\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fit the tree to data.\n",
        "        \"\"\"\n",
        "        # Accept only np.array or convert\n",
        "        if isinstance(X, list):\n",
        "            X = np.array(X)\n",
        "        if isinstance(y, list):\n",
        "            y = np.array(y)\n",
        "        if self.max_features is None:\n",
        "            self.max_features = int(np.sqrt(X.shape[1]))\n",
        "        self.tree = self.build_tree(X, y, 0)\n",
        "\n",
        "    def predict_sample(self, x, node):\n",
        "        \"\"\"\n",
        "        Predict a single sample recursively.\n",
        "        \"\"\"\n",
        "        if node['leaf']:\n",
        "            return node['class']\n",
        "        if x[node['feature']] < node['thresh']:\n",
        "            return self.predict_sample(x, node['left'])\n",
        "        else:\n",
        "            return self.predict_sample(x, node['right'])\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict all samples.\n",
        "        \"\"\"\n",
        "        if isinstance(X, list):\n",
        "            X = np.array(X)\n",
        "        return np.array([self.predict_sample(x, self.tree) for x in X])\n",
        "\n",
        "def collect_tree_importances(node, importances):\n",
        "    if node is None or node.get('leaf', False):\n",
        "        return\n",
        "    feature = node['feature']\n",
        "    gain = node.get('gain', 0)\n",
        "    importances[feature] += gain\n",
        "    collect_tree_importances(node['left'], importances)\n",
        "    collect_tree_importances(node['right'], importances)\n",
        "\n"
      ],
      "metadata": {
        "id": "vVA3aYkb6RT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3.2 RANDOM FOREST CLASSIFIER IMPLEMENTATION\n",
        "We implement a Random Forest classifier from scratch. This ensemble approach trains multiple decision trees on different bootstrap samples of the training data, and aggregates their predictions by majority voting. Random Forests help reduce overfitting compared to a single decision tree and provide a robust estimate of feature importance.\n",
        "\n",
        "In this section:\n",
        "1. Each tree is built using random subsets of the data and features.\n",
        "2. The final prediction is based on the most common outcome predicted by all trees.\n",
        "3. Hyperparameters such as the number of trees (`n_estimators`), maximum tree depth (`max_depth`), and minimum samples required for a split (`min_samples_split`) are tuned to optimize performance."
      ],
      "metadata": {
        "id": "yPZSmwvosGHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RANDOM FOREST CLASSIFIER IMPLEMENTATION\n",
        "class RandomForest:\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    - n_estimators: Number of trees in the forest\n",
        "    - max_depth: Maximum depth of each tree\n",
        "    - min_samples_split: Minimum samples for a split\n",
        "    - max_features: 'sqrt' for number of features per split\n",
        "  \"\"\"\n",
        "  def __init__(self,n_estimators = 100,max_depth=10,min_samples_split=2,max_features=2):\n",
        "    self.n_estimators = n_estimators\n",
        "    self.max_depth = max_depth\n",
        "    self.min_samples_split =  min_samples_split\n",
        "    self.max_features = max_features\n",
        "    self.trees = []\n",
        "\n",
        "\n",
        "  #bootstrap sampling\n",
        "  def _bootstrap_samples(self,X, y):\n",
        "      n_samples = X.shape[0]\n",
        "      idxs = np.random.choice(n_samples,n_samples,replace=True)\n",
        "      return X[idxs],y[idxs]\n",
        "\n",
        "  def _most_common_label(self,y):\n",
        "        counter = Counter(y)\n",
        "        most_common = counter.most_common(1)[0][0]\n",
        "        return most_common\n",
        "\n",
        "  def random_feature_selection(self, n_features):\n",
        "        if self.max_features == 'sqrt':\n",
        "            return max(1, int(np.sqrt(n_features)))\n",
        "        else:\n",
        "            return n_features\n",
        "\n",
        "  def fit(self,X,y):\n",
        "      self.trees = []\n",
        "      n_features = X.shape[1]\n",
        "      n_feats_per_split = self.random_feature_selection(n_features)\n",
        "      for _ in range(self.n_estimators):\n",
        "        tree = DecisionTree(max_depth = self.max_depth,\n",
        "                    min_samples_split = self.min_samples_split,\n",
        "                            max_features = n_feats_per_split)\n",
        "        X_sample,y_sample = self._bootstrap_samples(X,y)\n",
        "        tree.fit(X_sample,y_sample)\n",
        "        self.trees.append(tree)\n",
        "\n",
        "  def predict(self,X,):\n",
        "      predictions =  np.array([tree.predict(X) for tree in self.trees])\n",
        "      \"\"\"\n",
        "      [[s1t1,s2t1,s3t1],[s1t2,s2t2,s3t2],[s1t3],s2t3,s3t3]\n",
        "      \"\"\"\n",
        "      tree_preds = np.swapaxes(predictions,0,1)\n",
        "      \"\"\"\n",
        "      [[s1t1,s1t2,s1t3],[s2t1,s2t2,s2t3],[s3t1,s3t2,s3t3]]\n",
        "      \"\"\"\n",
        "      predictions = np.array([self._most_common_label(pred) for pred in tree_preds])\n",
        "      return predictions"
      ],
      "metadata": {
        "id": "Z6eyAT0BsE-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Model Evaluation and Hyperparameter Tuning\n",
        "\n",
        "###4.1 Model Testing\n"
      ],
      "metadata": {
        "id": "_XrODjE3SncI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X=train.drop('Survived',axis=1).values\n",
        "y=train['Survived'].values\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "\n",
        "def accuracy(y_true,y_pred):\n",
        "  accuracy = np.sum(y_true == y_pred)/len(y_true)\n",
        "  return accuracy\n",
        "\n",
        "clf=RandomForest()\n",
        "clf.fit(X_train,y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "acc = accuracy(y_test,predictions)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "n_features = X_train.shape[1]    # Set to the number of input features (not including target)\n",
        "\n",
        "forest_importances = np.zeros(n_features)\n",
        "for tree in clf.trees:\n",
        "    importances = np.zeros(n_features)\n",
        "    collect_tree_importances(tree.tree, importances)\n",
        "    forest_importances += importances\n",
        "\n",
        "forest_importances /= forest_importances.sum()\n",
        "\n",
        "# printing importances for each feature in order:\n",
        "feature_names = train.drop('Survived', axis=1).columns.tolist()\n",
        "\n",
        "#making a list of important features\n",
        "feature_importance_pairs = list(zip(feature_names, forest_importances))\n",
        "feature_importance_pairs.sort(key=lambda x: x[1], reverse=True) #sorting the list\n",
        "\n",
        "#setting N=5 for top 5 features to display\n",
        "N=5\n",
        "print(f\"\\nTop {N} Most Important Features:\")\n",
        "for feat, score in feature_importance_pairs[:N]:\n",
        "    print(f\"{feat}: {score:.3f}\")"
      ],
      "metadata": {
        "id": "UN0yD6aZSmas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.1 Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "ICvYnchraeFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "# Defining the hyperparameter grid\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "max_depth_list = [3, 5, 10, None]\n",
        "min_samples_split_list = [2, 5, 10]\n",
        "\n",
        "# Preparing to track results\n",
        "results = []\n",
        "\n",
        "# Looping through all combinations\n",
        "for n_estimators, max_depth, min_samples_split in itertools.product(n_estimators_list, max_depth_list, min_samples_split_list):\n",
        "\n",
        "    # # Print which set is running\n",
        "    # print(f\"Training: n_estimators={n_estimators}, max_depth={max_depth}, min_samples_split={min_samples_split}\")\n",
        "\n",
        "    rf = RandomForest(n_estimators = n_estimators,\n",
        "                      max_depth = max_depth,\n",
        "                      min_samples_split = min_samples_split)\n",
        "    rf.fit(X_train, y_train)\n",
        "    preds = rf.predict(X_test)\n",
        "\n",
        "    #calculating accuracy\n",
        "    accuracy = np.mean(preds == y_test)\n",
        "\n",
        "    #storing in results\n",
        "    results.append({'n_estimators': n_estimators,\n",
        "                    'max_depth': max_depth,\n",
        "                    'min_samples_split': min_samples_split,\n",
        "                    'accuracy': accuracy})\n",
        "\n",
        "#converting to dataframe\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "#sorting by accuracy\n",
        "df_results = df_results.sort_values(by='accuracy', ascending=False)\n",
        "\n",
        "print(df_results.head(5))"
      ],
      "metadata": {
        "id": "jWtQ4ztualXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2 Performance Metrics Implementation"
      ],
      "metadata": {
        "id": "mp2El1NvjFSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = clf.predict(X_test)\n",
        "\n",
        "#Accuracy\n",
        "def accuracy_score(y_test, y_test_pred):\n",
        "    \"\"\"\n",
        "    Returns the accuracy of the predictions.\n",
        "    \"\"\"\n",
        "    return np.sum(y_test == y_test_pred) / len(y_test)\n",
        "\n",
        "#Confusion Matrix\n",
        "def confusion_matrix(y_test, y_test_pred):\n",
        "    \"\"\"\n",
        "    Returns the confusion matrix in the form:\n",
        "        [[TN, FP],\n",
        "         [FN, TP]]\n",
        "    for binary classification (0 = negative, 1 = positive).\n",
        "    \"\"\"\n",
        "    tn = np.sum((y_test == 0) & (y_test_pred == 0))\n",
        "    fp = np.sum((y_test == 0) & (y_test_pred == 1))\n",
        "    fn = np.sum((y_test == 1) & (y_test_pred == 0))\n",
        "    tp = np.sum((y_test == 1) & (y_test_pred == 1))\n",
        "    return np.array([[tn, fp],\n",
        "                     [fn, tp]])\n",
        "\n",
        "\n",
        "#Precision,Recall,F1 score\n",
        "def precision_recall_f1(y_test, y_test_pred):\n",
        "    cm = confusion_matrix(y_test, y_test_pred)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    precision = tp / (tp + fp) if (tp + fp) else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) else 0\n",
        "    return precision, recall, f1\n",
        "\n",
        "\n",
        "acc = accuracy_score(y_test, y_test_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "print(f\"Confusion Matrix:\\n{cm}\")\n",
        "\n",
        "precision, recall, f1 = precision_recall_f1(y_test, y_test_pred)\n",
        "print(f\"Precision: {precision:.4f}  Recall: {recall:.4f}  F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "AOiP0y0hjIj6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}